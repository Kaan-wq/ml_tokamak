{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Kaan-wq/ml_tokamak/blob/main/NN_general.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wnJP6X6bVhZQ",
    "outputId": "30fb24eb-636d-4f95-f7c5-402321f67a15"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.patches as mpatches\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from typing import Iterable\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from scipy.fft import fft, ifft\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import keras\n",
    "import keras_tuner as kt\n",
    "from kerastuner import RandomSearch\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**I - Preprocessing of the data**\n",
    "\n",
    "Below is the preprocessing pipeline of the data. \\\\\n",
    "Essentially, we feature engineer a few columns, normalize the data and finally, we split it into a training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "louv_dDDVk0h"
   },
   "outputs": [],
   "source": [
    "# load dataset using pickle\n",
    "import pickle\n",
    "with open(\"../data/dataset_disruption_characterization.pickle\", \"rb\") as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ME0mg-qbzWRC"
   },
   "source": [
    "Here we load the data from the drive and put it into a more practical **data structure**. \\\\\n",
    "We add a column <code>['IPE']</code> which represents the current difference between the reference and actual currents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "xhsprZ_wi8-9",
    "outputId": "02ca27a5-c7ee-46b8-b5b2-6f7abc478f1f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IPLA</th>\n",
       "      <th>IPref</th>\n",
       "      <th>ECEcore</th>\n",
       "      <th>SSXcore</th>\n",
       "      <th>LI</th>\n",
       "      <th>Q95</th>\n",
       "      <th>ZMAG</th>\n",
       "      <th>Vloop</th>\n",
       "      <th>IPE</th>\n",
       "      <th>IPLA_fft_abs</th>\n",
       "      <th>...</th>\n",
       "      <th>LI_der2</th>\n",
       "      <th>Q95_der2</th>\n",
       "      <th>ZMAG_der2</th>\n",
       "      <th>Vloop_der2</th>\n",
       "      <th>Time</th>\n",
       "      <th>Frame</th>\n",
       "      <th>Event</th>\n",
       "      <th>Label</th>\n",
       "      <th>Shot</th>\n",
       "      <th>Window</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1989456.750</td>\n",
       "      <td>1999500.0</td>\n",
       "      <td>1740.929077</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>1.191489</td>\n",
       "      <td>3.874169</td>\n",
       "      <td>0.30388</td>\n",
       "      <td>-0.519496</td>\n",
       "      <td>10043.250</td>\n",
       "      <td>3.978742e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.019993</td>\n",
       "      <td>10.361</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>81206</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1989606.250</td>\n",
       "      <td>1999500.0</td>\n",
       "      <td>1744.737427</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>1.191489</td>\n",
       "      <td>3.874169</td>\n",
       "      <td>0.30388</td>\n",
       "      <td>-0.758418</td>\n",
       "      <td>9893.750</td>\n",
       "      <td>2.153723e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.119461</td>\n",
       "      <td>10.362</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>81206</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1988484.000</td>\n",
       "      <td>1999500.0</td>\n",
       "      <td>1756.823730</td>\n",
       "      <td>0.008698</td>\n",
       "      <td>1.191489</td>\n",
       "      <td>3.874169</td>\n",
       "      <td>0.30388</td>\n",
       "      <td>-1.037327</td>\n",
       "      <td>11016.000</td>\n",
       "      <td>4.264020e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.448103</td>\n",
       "      <td>10.363</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>81206</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1989329.625</td>\n",
       "      <td>1999500.0</td>\n",
       "      <td>1756.823730</td>\n",
       "      <td>0.008469</td>\n",
       "      <td>1.191489</td>\n",
       "      <td>3.874169</td>\n",
       "      <td>0.30388</td>\n",
       "      <td>-0.758418</td>\n",
       "      <td>10170.375</td>\n",
       "      <td>5.440219e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.159448</td>\n",
       "      <td>10.364</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>81206</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990532.250</td>\n",
       "      <td>1999500.0</td>\n",
       "      <td>1746.057251</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>1.191489</td>\n",
       "      <td>3.874169</td>\n",
       "      <td>0.30388</td>\n",
       "      <td>0.237256</td>\n",
       "      <td>8967.750</td>\n",
       "      <td>4.025099e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.537741</td>\n",
       "      <td>10.365</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>81206</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17975</th>\n",
       "      <td>3993662.500</td>\n",
       "      <td>4000500.0</td>\n",
       "      <td>6282.263672</td>\n",
       "      <td>1.416517</td>\n",
       "      <td>0.808256</td>\n",
       "      <td>2.898196</td>\n",
       "      <td>0.32961</td>\n",
       "      <td>-0.316564</td>\n",
       "      <td>6837.500</td>\n",
       "      <td>1.188296e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.138705</td>\n",
       "      <td>8.622</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>98005</td>\n",
       "      <td>905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17976</th>\n",
       "      <td>3995614.250</td>\n",
       "      <td>4000500.0</td>\n",
       "      <td>6303.055664</td>\n",
       "      <td>1.410871</td>\n",
       "      <td>0.808256</td>\n",
       "      <td>2.898196</td>\n",
       "      <td>0.32961</td>\n",
       "      <td>-0.237256</td>\n",
       "      <td>4885.750</td>\n",
       "      <td>6.513881e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.198435</td>\n",
       "      <td>8.623</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>98005</td>\n",
       "      <td>905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17977</th>\n",
       "      <td>3994348.000</td>\n",
       "      <td>4000500.0</td>\n",
       "      <td>6277.216309</td>\n",
       "      <td>1.402783</td>\n",
       "      <td>0.808256</td>\n",
       "      <td>2.898196</td>\n",
       "      <td>0.32961</td>\n",
       "      <td>-0.238922</td>\n",
       "      <td>6152.000</td>\n",
       "      <td>9.370845e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>8.624</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>98005</td>\n",
       "      <td>905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17978</th>\n",
       "      <td>3997734.500</td>\n",
       "      <td>4000500.0</td>\n",
       "      <td>6286.124512</td>\n",
       "      <td>1.391034</td>\n",
       "      <td>0.808256</td>\n",
       "      <td>2.898196</td>\n",
       "      <td>0.32961</td>\n",
       "      <td>0.279242</td>\n",
       "      <td>2765.500</td>\n",
       "      <td>1.626988e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.348553</td>\n",
       "      <td>8.625</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>98005</td>\n",
       "      <td>905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17979</th>\n",
       "      <td>3995231.500</td>\n",
       "      <td>4000500.0</td>\n",
       "      <td>6317.835449</td>\n",
       "      <td>1.415449</td>\n",
       "      <td>0.808256</td>\n",
       "      <td>2.898196</td>\n",
       "      <td>0.32961</td>\n",
       "      <td>-0.159614</td>\n",
       "      <td>5268.500</td>\n",
       "      <td>1.818775e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.478510</td>\n",
       "      <td>8.626</td>\n",
       "      <td>19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>98005</td>\n",
       "      <td>905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17980 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              IPLA      IPref      ECEcore   SSXcore        LI       Q95  \\\n",
       "0      1989456.750  1999500.0  1740.929077  0.008850  1.191489  3.874169   \n",
       "1      1989606.250  1999500.0  1744.737427  0.008850  1.191489  3.874169   \n",
       "2      1988484.000  1999500.0  1756.823730  0.008698  1.191489  3.874169   \n",
       "3      1989329.625  1999500.0  1756.823730  0.008469  1.191489  3.874169   \n",
       "4      1990532.250  1999500.0  1746.057251  0.008850  1.191489  3.874169   \n",
       "...            ...        ...          ...       ...       ...       ...   \n",
       "17975  3993662.500  4000500.0  6282.263672  1.416517  0.808256  2.898196   \n",
       "17976  3995614.250  4000500.0  6303.055664  1.410871  0.808256  2.898196   \n",
       "17977  3994348.000  4000500.0  6277.216309  1.402783  0.808256  2.898196   \n",
       "17978  3997734.500  4000500.0  6286.124512  1.391034  0.808256  2.898196   \n",
       "17979  3995231.500  4000500.0  6317.835449  1.415449  0.808256  2.898196   \n",
       "\n",
       "          ZMAG     Vloop        IPE  IPLA_fft_abs  ...  LI_der2  Q95_der2  \\\n",
       "0      0.30388 -0.519496  10043.250  3.978742e+07  ...      0.0       0.0   \n",
       "1      0.30388 -0.758418   9893.750  2.153723e+03  ...      0.0       0.0   \n",
       "2      0.30388 -1.037327  11016.000  4.264020e+03  ...      0.0       0.0   \n",
       "3      0.30388 -0.758418  10170.375  5.440219e+03  ...      0.0       0.0   \n",
       "4      0.30388  0.237256   8967.750  4.025099e+03  ...      0.0       0.0   \n",
       "...        ...       ...        ...           ...  ...      ...       ...   \n",
       "17975  0.32961 -0.316564   6837.500  1.188296e+04  ...      0.0       0.0   \n",
       "17976  0.32961 -0.237256   4885.750  6.513881e+03  ...      0.0       0.0   \n",
       "17977  0.32961 -0.238922   6152.000  9.370845e+03  ...      0.0       0.0   \n",
       "17978  0.32961  0.279242   2765.500  1.626988e+04  ...      0.0       0.0   \n",
       "17979  0.32961 -0.159614   5268.500  1.818775e+04  ...      0.0       0.0   \n",
       "\n",
       "       ZMAG_der2  Vloop_der2    Time  Frame  Event  Label   Shot  Window  \n",
       "0            0.0   -0.019993  10.361      0    0.0      0  81206       0  \n",
       "1            0.0    0.119461  10.362      1    0.0      0  81206       0  \n",
       "2            0.0    0.448103  10.363      2    0.0      0  81206       0  \n",
       "3            0.0   -0.159448  10.364      3    0.0      0  81206       0  \n",
       "4            0.0   -0.537741  10.365      4    0.0      0  81206       0  \n",
       "...          ...         ...     ...    ...    ...    ...    ...     ...  \n",
       "17975        0.0    0.138705   8.622     15    0.0      0  98005     905  \n",
       "17976        0.0    0.198435   8.623     16    0.0      0  98005     905  \n",
       "17977        0.0    0.000416   8.624     17    0.0      0  98005     905  \n",
       "17978        0.0   -0.348553   8.625     18    0.0      0  98005     905  \n",
       "17979        0.0   -0.478510   8.626     19    0.0      0  98005     905  \n",
       "\n",
       "[17980 rows x 42 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels count:\n",
      "Label\n",
      "0    12840\n",
      "1     4140\n",
      "2     1000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_data = pd.DataFrame()\n",
    "\n",
    "# Loop through each entry in the dataset\n",
    "for i, entry in enumerate(dataset):\n",
    "    # Extract data and label from the current entry\n",
    "    d = entry['x']\n",
    "    label = entry['y']\n",
    "    metadata = entry['metadata']\n",
    "    event = metadata['time_event']\n",
    "\n",
    "    # Create a DataFrame for the current entry\n",
    "    df = pd.DataFrame(d['data'], columns=d['columns'])\n",
    "\n",
    "    # Add the IPE column\n",
    "    df['IPE'] = np.abs(df['IPLA'] - df['IPref'])\n",
    "\n",
    "    # Define columns to perform feeture engineering on\n",
    "    column = ['IPLA', 'IPref', 'IPE', 'ECEcore', 'SSXcore', 'LI', 'Q95', 'ZMAG', 'Vloop']\n",
    "\n",
    "    # Add the Fourier Transform columns\n",
    "    for col in column:\n",
    "      # Perform the Fourier Transform\n",
    "      df[col + '_fft'] = np.fft.fft(df[col])\n",
    "\n",
    "      # The result is complex numbers, you might want to take absolute value (magnitude) of the result\n",
    "      df[col + '_fft_abs'] = np.abs(df[col + '_fft'])\n",
    "\n",
    "      # Drop the column with complex numbers\n",
    "      df = df.drop(columns=[col + '_fft'])\n",
    "\n",
    "\n",
    "    # Add the first order derivative columns\n",
    "    for col in column:\n",
    "      # Perform the first order derivative\n",
    "      df[col + '_der'] = np.gradient(df[col])\n",
    "\n",
    "    # Add the second order derivative columns\n",
    "    for col in column:\n",
    "      # Perform the second order derivative\n",
    "      df[col + '_der2'] = np.gradient(df[col + '_der'])\n",
    "    \n",
    "    # Add the time column\n",
    "    df['Time'] = d['time']\n",
    "\n",
    "    # Add the Frame, Event, Label, Shot and Window columns\n",
    "    df['Frame'] = range(0, 20)\n",
    "    df['Event'] = event if event else 0\n",
    "\n",
    "    if event:\n",
    "      #Find closest points to time_event\n",
    "      differences = np.abs(df['Time'] - event)\n",
    "      closest_indices = np.argsort(differences)[:20]\n",
    "\n",
    "      #Assign labels to closest points\n",
    "      df['Label'] = 0\n",
    "      df.loc[closest_indices, 'Label'] = label\n",
    "    else:\n",
    "      df['Label'] = label\n",
    "\n",
    "    df['Shot'] = metadata['shot']\n",
    "    df['Window'] = i  # Add the window number\n",
    "\n",
    "    contains_nan = df.isna().any().any()\n",
    "\n",
    "    if not contains_nan:\n",
    "      # Append the current DataFrame to the main DataFrame\n",
    "      df_data = pd.concat([df_data, df], ignore_index=True)\n",
    "\n",
    "\n",
    "df_data = df_data.dropna()\n",
    "display(df_data)\n",
    "print(\"Labels count:\")\n",
    "print(f\"{df_data['Label'].value_counts()}\")\n",
    "\n",
    "base_col = df_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "shot = df_data[df_data['Label'] == 2]['Shot'].unique()[6]\n",
    "\n",
    "# Filter the DataFrame to only include a specific shot\n",
    "df_shot = df_data[df_data['Shot'] == shot]\n",
    "\n",
    "# Filter the DataFrame to only include rows with labels clustered\n",
    "df_label_2 = df_shot[df_shot['Label'] == 2]\n",
    "df_label_1 = df_shot[df_shot['Label'] == 1]\n",
    "df_label_0 = df_shot[df_shot['Label'] == 0]\n",
    "\n",
    "# Select a specific window for each label\n",
    "window_number_2 = df_label_2['Window'].unique()[0] \n",
    "df_window_2 = df_label_2[df_label_2['Window'] == window_number_2]\n",
    "\n",
    "window_number_1 = df_label_1['Window'].unique()[0] \n",
    "df_window_1 = df_label_1[df_label_1['Window'] == window_number_1]\n",
    "\n",
    "window_number_0 = df_label_0['Window'].unique()[0]\n",
    "df_window_0 = df_label_0[df_label_0['Window'] == window_number_0]\n",
    "\n",
    "# Plot each feature across time\n",
    "features = df_data.columns.drop(['Label', 'Window', 'Shot', 'Frame', 'Time', 'Event'])  # Exclude non-feature columns\n",
    "\n",
    "# Print the time of the event\n",
    "print(\"Time of the event for each label is :\")\n",
    "print(f\"Label 2: {df_window_2['Event'].unique()[0]}\")\n",
    "print(f\"Label 1: {df_window_1['Event'].unique()[0]}\")\n",
    "print(f\"Label 0: {df_window_0['Event'].unique()[0]}\")\n",
    "\n",
    "# Create a subplot for each feature\n",
    "fig, axs = plt.subplots(len(features), 3, figsize=(20, 6*len(features)))\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    axs[i, 0].plot(df_window_2['Time'], df_window_2[feature], label=f'Label 2')\n",
    "    axs[i, 0].set_title(f'Feature: {feature} over Time for Window: {window_number_2}')\n",
    "    axs[i, 0].set_xlabel('Time')\n",
    "    axs[i, 0].set_ylabel(feature)\n",
    "    axs[i, 0].legend()\n",
    "\n",
    "    axs[i, 1].plot(df_window_1['Time'], df_window_1[feature], label=f'Label 1')\n",
    "    axs[i, 1].set_title(f'Feature: {feature} over Time for Window: {window_number_1}')\n",
    "    axs[i, 1].set_xlabel('Time')\n",
    "    axs[i, 1].set_ylabel(feature)\n",
    "    axs[i, 1].legend()\n",
    "\n",
    "    axs[i, 2].plot(df_window_0['Time'], df_window_0[feature], label=f'Label 0')\n",
    "    axs[i, 2].set_title(f'Feature: {feature} over Time for Window: {window_number_0}')\n",
    "    axs[i, 2].set_xlabel('Time')\n",
    "    axs[i, 2].set_ylabel(feature)\n",
    "    axs[i, 2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the plot in an image file\n",
    "fig.savefig('../data/feature_time.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label              1.000\n",
       "Vloop_fft_abs      0.559\n",
       "IPLA               0.309\n",
       "IPLA_der2          0.212\n",
       "Vloop_der2         0.203\n",
       "Vloop              0.187\n",
       "IPref              0.185\n",
       "IPLA_der           0.181\n",
       "Vloop_der          0.173\n",
       "IPE_der2           0.065\n",
       "IPE_der            0.054\n",
       "IPE                0.037\n",
       "IPLA_fft_abs       0.036\n",
       "IPref_fft_abs      0.022\n",
       "IPE_fft_abs        0.014\n",
       "ECEcore_fft_abs    0.013\n",
       "IPref_der          0.012\n",
       "LI_der2            0.008\n",
       "LI_der             0.008\n",
       "ZMAG_der           0.001\n",
       "ZMAG_der2         -0.001\n",
       "IPref_der2        -0.002\n",
       "SSXcore_fft_abs   -0.004\n",
       "LI_fft_abs        -0.005\n",
       "Q95_der2          -0.005\n",
       "ZMAG_fft_abs      -0.013\n",
       "LI                -0.018\n",
       "Q95_fft_abs       -0.022\n",
       "Q95_der           -0.025\n",
       "ZMAG              -0.057\n",
       "SSXcore_der2      -0.071\n",
       "SSXcore_der       -0.081\n",
       "SSXcore           -0.086\n",
       "ECEcore           -0.093\n",
       "ECEcore_der2      -0.108\n",
       "ECEcore_der       -0.146\n",
       "Q95               -0.268\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_disr = df_data[df_data['Label'] != 0].drop(columns=['Window', 'Shot', 'Frame', 'Time', 'Event'])\n",
    "df_disr.corr()['Label'].round(3).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dGguf_6tsM9S",
    "outputId": "52fcd99e-e6f2-43b8-db2f-9eac43f6d709"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['IPLA', 'IPref', 'ECEcore', 'SSXcore', 'LI', 'Q95', 'ZMAG', 'Vloop',\n",
       "       'IPE', 'IPLA_fft_abs', 'IPref_fft_abs', 'IPE_fft_abs',\n",
       "       'ECEcore_fft_abs', 'SSXcore_fft_abs', 'LI_fft_abs', 'Q95_fft_abs',\n",
       "       'ZMAG_fft_abs', 'Vloop_fft_abs', 'IPLA_der', 'IPref_der', 'IPE_der',\n",
       "       'ECEcore_der', 'SSXcore_der', 'LI_der', 'Q95_der', 'ZMAG_der',\n",
       "       'Vloop_der', 'IPLA_der2', 'IPref_der2', 'IPE_der2', 'ECEcore_der2',\n",
       "       'SSXcore_der2', 'LI_der2', 'Q95_der2', 'ZMAG_der2', 'Vloop_der2',\n",
       "       'Time', 'Frame', 'Event', 'Label', 'Shot', 'Window'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6qCEtg82u00"
   },
   "source": [
    "We **normalize and split** into training and test sets before feeding it into our **Neural Network**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g_qkjUMio62W",
    "outputId": "d02726ec-3769-4059-c375-a6cec7dad4ab"
   },
   "outputs": [],
   "source": [
    "# Split 'Shot' values into training and test sets and create DataFrames\n",
    "shot_train, shot_test = train_test_split(df_data['Shot'].unique(), test_size=0.2, random_state=42)\n",
    "train_df = df_data[df_data['Shot'].isin(shot_train)]\n",
    "test_df = df_data[df_data['Shot'].isin(shot_test)]\n",
    "\n",
    "# Group by 'Window' and 'Shot', and reshape the data\n",
    "train_df_grouped = train_df.sort_values(['Shot', 'Window', 'Time']).groupby(['Shot', 'Window'])\n",
    "test_df_grouped = test_df.sort_values(['Shot', 'Window', 'Time']).groupby(['Shot', 'Window'])\n",
    "\n",
    "# Prepare lists to hold sequences\n",
    "X_train_grouped, y_train_grouped = [], []\n",
    "X_test_grouped, y_test_grouped = [], []\n",
    "\n",
    "# Generate sequences for training data\n",
    "for _, group in train_df_grouped:\n",
    "    X_train_grouped.append(group.drop(columns=['Frame', 'Event', 'Label', 'Shot', 'Window']).values)\n",
    "    y_train_grouped.append(group['Label'].values[0])  # Assuming all labels in a group are the same\n",
    "\n",
    "# Generate sequences for testing data\n",
    "for _, group in test_df_grouped:\n",
    "    X_test_grouped.append(group.drop(columns=['Frame', 'Event', 'Label', 'Shot', 'Window']).values)\n",
    "    y_test_grouped.append(group['Label'].values[0])  # Assuming all labels in a group are the same\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_train_grouped = np.array(X_train_grouped)\n",
    "y_train_grouped = np.array(y_train_grouped)\n",
    "X_test_grouped = np.array(X_test_grouped)\n",
    "y_test_grouped = np.array(y_test_grouped)\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train_grouped.reshape(-1, X_train_grouped.shape[-1])).reshape(X_train_grouped.shape)\n",
    "X_test = scaler.transform(X_test_grouped.reshape(-1, X_test_grouped.shape[-1])).reshape(X_test_grouped.shape)\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_train = to_categorical(y_train_grouped)\n",
    "y_test = to_categorical(y_test_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(737, 20, 37)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Z-zSjmr3ABP"
   },
   "source": [
    "#**II - Model tuning**\n",
    "\n",
    "Here we use the hyperparamter tuner from keras to find the best model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HngMK-Dgv6VR",
    "outputId": "cf3f47ac-938f-4711-8629-a6266d63c465"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 20, 37)]             0         []                            \n",
      "                                                                                                  \n",
      " conv1d_25 (Conv1D)          (None, 20, 64)               19008     ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_18 (Ba  (None, 20, 64)               256       ['conv1d_25[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_18 (Activation)  (None, 20, 64)               0         ['batch_normalization_18[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_26 (Conv1D)          (None, 20, 64)               32832     ['activation_18[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_19 (Ba  (None, 20, 64)               256       ['conv1d_26[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_19 (Activation)  (None, 20, 64)               0         ['batch_normalization_19[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_27 (Conv1D)          (None, 20, 64)               32832     ['activation_19[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_20 (Ba  (None, 20, 64)               256       ['conv1d_27[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_20 (Activation)  (None, 20, 64)               0         ['batch_normalization_20[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_24 (Conv1D)          (None, 20, 64)               2432      ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " add_6 (Add)                 (None, 20, 64)               0         ['activation_20[0][0]',       \n",
      "                                                                     'conv1d_24[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_29 (Conv1D)          (None, 20, 64)               20544     ['add_6[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_21 (Ba  (None, 20, 64)               256       ['conv1d_29[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_21 (Activation)  (None, 20, 64)               0         ['batch_normalization_21[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_30 (Conv1D)          (None, 20, 64)               20544     ['activation_21[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_22 (Ba  (None, 20, 64)               256       ['conv1d_30[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_22 (Activation)  (None, 20, 64)               0         ['batch_normalization_22[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_31 (Conv1D)          (None, 20, 64)               20544     ['activation_22[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_23 (Ba  (None, 20, 64)               256       ['conv1d_31[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_23 (Activation)  (None, 20, 64)               0         ['batch_normalization_23[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_28 (Conv1D)          (None, 20, 64)               4160      ['add_6[0][0]']               \n",
      "                                                                                                  \n",
      " add_7 (Add)                 (None, 20, 64)               0         ['activation_23[0][0]',       \n",
      "                                                                     'conv1d_28[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_33 (Conv1D)          (None, 20, 64)               12352     ['add_7[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_24 (Ba  (None, 20, 64)               256       ['conv1d_33[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_24 (Activation)  (None, 20, 64)               0         ['batch_normalization_24[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_34 (Conv1D)          (None, 20, 64)               12352     ['activation_24[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_25 (Ba  (None, 20, 64)               256       ['conv1d_34[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_25 (Activation)  (None, 20, 64)               0         ['batch_normalization_25[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_35 (Conv1D)          (None, 20, 64)               12352     ['activation_25[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_26 (Ba  (None, 20, 64)               256       ['conv1d_35[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_26 (Activation)  (None, 20, 64)               0         ['batch_normalization_26[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_32 (Conv1D)          (None, 20, 64)               4160      ['add_7[0][0]']               \n",
      "                                                                                                  \n",
      " add_8 (Add)                 (None, 20, 64)               0         ['activation_26[0][0]',       \n",
      "                                                                     'conv1d_32[0][0]']           \n",
      "                                                                                                  \n",
      " global_average_pooling1d_2  (None, 64)                   0         ['add_8[0][0]']               \n",
      "  (GlobalAveragePooling1D)                                                                        \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 3)                    195       ['global_average_pooling1d_2[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 196611 (768.01 KB)\n",
      "Trainable params: 195459 (763.51 KB)\n",
      "Non-trainable params: 1152 (4.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, Add, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Function for creating a residual block\n",
    "def residual_block(x, filters, kernel_size):\n",
    "    # Save the input value. Apply a 1x1 convolution to match the number of channels.\n",
    "    residual = Conv1D(filters, 1, padding='same')(x)\n",
    "\n",
    "    # Convolution, batch normalization, and ReLU activation (repeated three times)\n",
    "    for _ in range(3):\n",
    "        x = Conv1D(filters, kernel_size, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "    # Add the residual (input) to the output\n",
    "    x = Add()([x, residual])\n",
    "\n",
    "    return x\n",
    "\n",
    "# Input shape: (timesteps, features)\n",
    "input_shape = (20, 37)\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=input_shape)\n",
    "\n",
    "# Create three residual blocks with varying kernel sizes\n",
    "x = residual_block(input_layer, 64, 8)\n",
    "x = residual_block(x, 64, 5)\n",
    "x = residual_block(x, 64, 3)\n",
    "\n",
    "# Global Average Pooling layer\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "# Final softmax classifier\n",
    "output_layer = Dense(3, activation='softmax')(x)\n",
    "\n",
    "# Create the model and compile it\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = Adam(learning_rate=0.0000001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy', tfa.metrics.F1Score(num_classes=3, average=None)])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "24/24 [==============================] - 5s 33ms/step - loss: 0.9118 - accuracy: 0.6187 - f1_score: 0.3743 - val_loss: 0.9313 - val_accuracy: 0.6605 - val_f1_score: 0.3756\n",
      "Epoch 2/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.9097 - accuracy: 0.6255 - f1_score: 0.3915 - val_loss: 0.9388 - val_accuracy: 0.6667 - val_f1_score: 0.3791\n",
      "Epoch 3/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.9213 - accuracy: 0.6174 - f1_score: 0.3831 - val_loss: 0.9298 - val_accuracy: 0.6667 - val_f1_score: 0.3498\n",
      "Epoch 4/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.9183 - accuracy: 0.6309 - f1_score: 0.4097 - val_loss: 0.9200 - val_accuracy: 0.7037 - val_f1_score: 0.3848\n",
      "Epoch 5/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8961 - accuracy: 0.6445 - f1_score: 0.3889 - val_loss: 0.9147 - val_accuracy: 0.7284 - val_f1_score: 0.3979\n",
      "Epoch 6/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.9088 - accuracy: 0.6391 - f1_score: 0.3976 - val_loss: 0.9144 - val_accuracy: 0.7407 - val_f1_score: 0.4050\n",
      "Epoch 7/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.9040 - accuracy: 0.6296 - f1_score: 0.3817 - val_loss: 0.9182 - val_accuracy: 0.7469 - val_f1_score: 0.4086\n",
      "Epoch 8/10000\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.8863 - accuracy: 0.6269 - f1_score: 0.3939 - val_loss: 0.9245 - val_accuracy: 0.7469 - val_f1_score: 0.4086\n",
      "Epoch 9/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8879 - accuracy: 0.6404 - f1_score: 0.3976 - val_loss: 0.9312 - val_accuracy: 0.7469 - val_f1_score: 0.4086\n",
      "Epoch 10/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8928 - accuracy: 0.6323 - f1_score: 0.3934 - val_loss: 0.9381 - val_accuracy: 0.7407 - val_f1_score: 0.4051\n",
      "Epoch 11/10000\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.8885 - accuracy: 0.6364 - f1_score: 0.3965 - val_loss: 0.9480 - val_accuracy: 0.7531 - val_f1_score: 0.4437\n",
      "Epoch 12/10000\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.8994 - accuracy: 0.6377 - f1_score: 0.4123 - val_loss: 0.9550 - val_accuracy: 0.7469 - val_f1_score: 0.4393\n",
      "Epoch 13/10000\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.8953 - accuracy: 0.6296 - f1_score: 0.3997 - val_loss: 0.9600 - val_accuracy: 0.7531 - val_f1_score: 0.4566\n",
      "Epoch 14/10000\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.9033 - accuracy: 0.6187 - f1_score: 0.3765 - val_loss: 0.9584 - val_accuracy: 0.7531 - val_f1_score: 0.4566\n",
      "Epoch 15/10000\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.8959 - accuracy: 0.6336 - f1_score: 0.3944 - val_loss: 0.9606 - val_accuracy: 0.7407 - val_f1_score: 0.4476\n",
      "Epoch 16/10000\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.8969 - accuracy: 0.6309 - f1_score: 0.3982 - val_loss: 0.9652 - val_accuracy: 0.7284 - val_f1_score: 0.4396\n",
      "Epoch 17/10000\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.8987 - accuracy: 0.6282 - f1_score: 0.3830 - val_loss: 0.9650 - val_accuracy: 0.7284 - val_f1_score: 0.4506\n",
      "Epoch 18/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8820 - accuracy: 0.6486 - f1_score: 0.4201 - val_loss: 0.9681 - val_accuracy: 0.7284 - val_f1_score: 0.4506\n",
      "Epoch 19/10000\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.8820 - accuracy: 0.6554 - f1_score: 0.4382 - val_loss: 0.9679 - val_accuracy: 0.7160 - val_f1_score: 0.4420\n",
      "Epoch 20/10000\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.8928 - accuracy: 0.6418 - f1_score: 0.3892 - val_loss: 0.9646 - val_accuracy: 0.7284 - val_f1_score: 0.4506\n",
      "Epoch 21/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8797 - accuracy: 0.6445 - f1_score: 0.4064 - val_loss: 0.9636 - val_accuracy: 0.7222 - val_f1_score: 0.4462\n",
      "Epoch 22/10000\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.8764 - accuracy: 0.6364 - f1_score: 0.3935 - val_loss: 0.9593 - val_accuracy: 0.7222 - val_f1_score: 0.4462\n",
      "Epoch 23/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8904 - accuracy: 0.6445 - f1_score: 0.4040 - val_loss: 0.9630 - val_accuracy: 0.7222 - val_f1_score: 0.4462\n",
      "Epoch 24/10000\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.8741 - accuracy: 0.6459 - f1_score: 0.3909 - val_loss: 0.9580 - val_accuracy: 0.7284 - val_f1_score: 0.4506\n",
      "Epoch 25/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8677 - accuracy: 0.6472 - f1_score: 0.4160 - val_loss: 0.9566 - val_accuracy: 0.7222 - val_f1_score: 0.4354\n",
      "Epoch 26/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8755 - accuracy: 0.6540 - f1_score: 0.4083 - val_loss: 0.9527 - val_accuracy: 0.7346 - val_f1_score: 0.4433\n",
      "Epoch 27/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8731 - accuracy: 0.6431 - f1_score: 0.4080 - val_loss: 0.9532 - val_accuracy: 0.7284 - val_f1_score: 0.4396\n",
      "Epoch 28/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8574 - accuracy: 0.6459 - f1_score: 0.4106 - val_loss: 0.9522 - val_accuracy: 0.7222 - val_f1_score: 0.4354\n",
      "Epoch 29/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8632 - accuracy: 0.6811 - f1_score: 0.4208 - val_loss: 0.9500 - val_accuracy: 0.7222 - val_f1_score: 0.4354\n",
      "Epoch 30/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8626 - accuracy: 0.6526 - f1_score: 0.4203 - val_loss: 0.9457 - val_accuracy: 0.7346 - val_f1_score: 0.4433\n",
      "Epoch 31/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8695 - accuracy: 0.6418 - f1_score: 0.3961 - val_loss: 0.9432 - val_accuracy: 0.7407 - val_f1_score: 0.4476\n",
      "Epoch 32/10000\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.8544 - accuracy: 0.6594 - f1_score: 0.4249 - val_loss: 0.9418 - val_accuracy: 0.7469 - val_f1_score: 0.4520\n",
      "Epoch 33/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8567 - accuracy: 0.6499 - f1_score: 0.4140 - val_loss: 0.9442 - val_accuracy: 0.7407 - val_f1_score: 0.4483\n",
      "Epoch 34/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8620 - accuracy: 0.6499 - f1_score: 0.4057 - val_loss: 0.9411 - val_accuracy: 0.7469 - val_f1_score: 0.4520\n",
      "Epoch 35/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8575 - accuracy: 0.6608 - f1_score: 0.4174 - val_loss: 0.9428 - val_accuracy: 0.7407 - val_f1_score: 0.4483\n",
      "Epoch 36/10000\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.8544 - accuracy: 0.6581 - f1_score: 0.4117 - val_loss: 0.9425 - val_accuracy: 0.7407 - val_f1_score: 0.4483\n",
      "Epoch 37/10000\n",
      "24/24 [==============================] - 0s 15ms/step - loss: 0.8519 - accuracy: 0.6703 - f1_score: 0.4286 - val_loss: 0.9369 - val_accuracy: 0.7407 - val_f1_score: 0.4483\n",
      "Epoch 38/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8576 - accuracy: 0.6581 - f1_score: 0.4147 - val_loss: 0.9361 - val_accuracy: 0.7407 - val_f1_score: 0.4483\n",
      "Epoch 39/10000\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.8430 - accuracy: 0.6649 - f1_score: 0.4350 - val_loss: 0.9313 - val_accuracy: 0.7469 - val_f1_score: 0.4520\n",
      "Epoch 40/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8471 - accuracy: 0.6554 - f1_score: 0.4220 - val_loss: 0.9260 - val_accuracy: 0.7469 - val_f1_score: 0.4514\n",
      "Epoch 41/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8404 - accuracy: 0.6513 - f1_score: 0.4015 - val_loss: 0.9242 - val_accuracy: 0.7469 - val_f1_score: 0.4514\n",
      "Epoch 42/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8322 - accuracy: 0.6594 - f1_score: 0.4194 - val_loss: 0.9217 - val_accuracy: 0.7531 - val_f1_score: 0.4559\n",
      "Epoch 43/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8410 - accuracy: 0.6608 - f1_score: 0.4087 - val_loss: 0.9191 - val_accuracy: 0.7531 - val_f1_score: 0.4559\n",
      "Epoch 44/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8456 - accuracy: 0.6459 - f1_score: 0.4202 - val_loss: 0.9162 - val_accuracy: 0.7593 - val_f1_score: 0.4599\n",
      "Epoch 45/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8379 - accuracy: 0.6703 - f1_score: 0.4376 - val_loss: 0.9153 - val_accuracy: 0.7593 - val_f1_score: 0.4599\n",
      "Epoch 46/10000\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.8463 - accuracy: 0.6581 - f1_score: 0.4198 - val_loss: 0.9114 - val_accuracy: 0.7593 - val_f1_score: 0.4599\n",
      "Epoch 47/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8318 - accuracy: 0.6649 - f1_score: 0.4168 - val_loss: 0.9085 - val_accuracy: 0.7593 - val_f1_score: 0.4599\n",
      "Epoch 48/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8392 - accuracy: 0.6716 - f1_score: 0.4257 - val_loss: 0.9108 - val_accuracy: 0.7531 - val_f1_score: 0.4430\n",
      "Epoch 49/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8257 - accuracy: 0.6581 - f1_score: 0.4211 - val_loss: 0.9061 - val_accuracy: 0.7531 - val_f1_score: 0.4430\n",
      "Epoch 50/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8224 - accuracy: 0.6621 - f1_score: 0.4159 - val_loss: 0.9065 - val_accuracy: 0.7531 - val_f1_score: 0.4430\n",
      "Epoch 51/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8241 - accuracy: 0.6784 - f1_score: 0.4555 - val_loss: 0.8925 - val_accuracy: 0.7593 - val_f1_score: 0.4599\n",
      "Epoch 52/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8251 - accuracy: 0.6567 - f1_score: 0.4158 - val_loss: 0.8949 - val_accuracy: 0.7593 - val_f1_score: 0.4599\n",
      "Epoch 53/10000\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.8306 - accuracy: 0.6825 - f1_score: 0.4453 - val_loss: 0.8940 - val_accuracy: 0.7531 - val_f1_score: 0.4430\n",
      "Epoch 54/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8253 - accuracy: 0.6811 - f1_score: 0.4422 - val_loss: 0.8933 - val_accuracy: 0.7531 - val_f1_score: 0.4430\n",
      "Epoch 55/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8179 - accuracy: 0.6635 - f1_score: 0.4291 - val_loss: 0.8920 - val_accuracy: 0.7531 - val_f1_score: 0.4430\n",
      "Epoch 56/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8270 - accuracy: 0.6703 - f1_score: 0.4314 - val_loss: 0.8898 - val_accuracy: 0.7531 - val_f1_score: 0.4430\n",
      "Epoch 57/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8213 - accuracy: 0.6784 - f1_score: 0.4375 - val_loss: 0.8890 - val_accuracy: 0.7531 - val_f1_score: 0.4430\n",
      "Epoch 58/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8220 - accuracy: 0.6730 - f1_score: 0.4432 - val_loss: 0.8896 - val_accuracy: 0.7531 - val_f1_score: 0.4430\n",
      "Epoch 59/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8158 - accuracy: 0.6798 - f1_score: 0.4424 - val_loss: 0.8893 - val_accuracy: 0.7531 - val_f1_score: 0.4430\n",
      "Epoch 60/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8185 - accuracy: 0.6771 - f1_score: 0.4460 - val_loss: 0.8897 - val_accuracy: 0.7531 - val_f1_score: 0.4430\n",
      "Epoch 61/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8048 - accuracy: 0.6784 - f1_score: 0.4302 - val_loss: 0.8862 - val_accuracy: 0.7531 - val_f1_score: 0.4430\n",
      "Epoch 62/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8076 - accuracy: 0.6879 - f1_score: 0.4451 - val_loss: 0.8839 - val_accuracy: 0.7531 - val_f1_score: 0.4430\n",
      "Epoch 63/10000\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.8154 - accuracy: 0.6784 - f1_score: 0.4521 - val_loss: 0.8797 - val_accuracy: 0.7531 - val_f1_score: 0.4430\n",
      "Epoch 64/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8033 - accuracy: 0.6811 - f1_score: 0.4357 - val_loss: 0.8760 - val_accuracy: 0.7531 - val_f1_score: 0.4430\n",
      "Epoch 65/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8070 - accuracy: 0.6879 - f1_score: 0.4572 - val_loss: 0.8737 - val_accuracy: 0.7531 - val_f1_score: 0.4430\n",
      "Epoch 66/10000\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.7904 - accuracy: 0.6879 - f1_score: 0.4510 - val_loss: 0.8726 - val_accuracy: 0.7593 - val_f1_score: 0.4472\n",
      "Epoch 67/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8015 - accuracy: 0.6893 - f1_score: 0.4629 - val_loss: 0.8702 - val_accuracy: 0.7593 - val_f1_score: 0.4472\n",
      "Epoch 68/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8010 - accuracy: 0.6852 - f1_score: 0.4413 - val_loss: 0.8693 - val_accuracy: 0.7593 - val_f1_score: 0.4472\n",
      "Epoch 69/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8069 - accuracy: 0.6811 - f1_score: 0.4415 - val_loss: 0.8668 - val_accuracy: 0.7593 - val_f1_score: 0.4472\n",
      "Epoch 70/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8042 - accuracy: 0.6784 - f1_score: 0.4400 - val_loss: 0.8677 - val_accuracy: 0.7593 - val_f1_score: 0.4472\n",
      "Epoch 71/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.7923 - accuracy: 0.6825 - f1_score: 0.4319 - val_loss: 0.8676 - val_accuracy: 0.7593 - val_f1_score: 0.4472\n",
      "Epoch 72/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.8010 - accuracy: 0.6906 - f1_score: 0.4534 - val_loss: 0.8646 - val_accuracy: 0.7593 - val_f1_score: 0.4472\n",
      "Epoch 73/10000\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.7915 - accuracy: 0.6988 - f1_score: 0.4635 - val_loss: 0.8584 - val_accuracy: 0.7593 - val_f1_score: 0.4472\n",
      "Epoch 74/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.7915 - accuracy: 0.7001 - f1_score: 0.4590 - val_loss: 0.8544 - val_accuracy: 0.7716 - val_f1_score: 0.4559\n",
      "Epoch 75/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.7804 - accuracy: 0.6961 - f1_score: 0.4839 - val_loss: 0.8518 - val_accuracy: 0.7716 - val_f1_score: 0.4559\n",
      "Epoch 76/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.7911 - accuracy: 0.6920 - f1_score: 0.4356 - val_loss: 0.8511 - val_accuracy: 0.7716 - val_f1_score: 0.4559\n",
      "Epoch 77/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.7887 - accuracy: 0.6852 - f1_score: 0.4545 - val_loss: 0.8491 - val_accuracy: 0.7716 - val_f1_score: 0.4559\n",
      "Epoch 78/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.7907 - accuracy: 0.6689 - f1_score: 0.4226 - val_loss: 0.8462 - val_accuracy: 0.7778 - val_f1_score: 0.4604\n",
      "Epoch 79/10000\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.7745 - accuracy: 0.7001 - f1_score: 0.4717 - val_loss: 0.8449 - val_accuracy: 0.7778 - val_f1_score: 0.4604\n",
      "Epoch 80/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.7809 - accuracy: 0.6920 - f1_score: 0.4363 - val_loss: 0.8426 - val_accuracy: 0.7778 - val_f1_score: 0.4604\n",
      "Epoch 81/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.7793 - accuracy: 0.7001 - f1_score: 0.4607 - val_loss: 0.8393 - val_accuracy: 0.7716 - val_f1_score: 0.4417\n",
      "Epoch 82/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.7741 - accuracy: 0.7096 - f1_score: 0.4830 - val_loss: 0.8354 - val_accuracy: 0.7778 - val_f1_score: 0.4604\n",
      "Epoch 83/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.7692 - accuracy: 0.6934 - f1_score: 0.4520 - val_loss: 0.8308 - val_accuracy: 0.7778 - val_f1_score: 0.4604\n",
      "Epoch 84/10000\n",
      "24/24 [==============================] - 0s 17ms/step - loss: 0.7795 - accuracy: 0.6934 - f1_score: 0.4637 - val_loss: 0.8170 - val_accuracy: 0.7963 - val_f1_score: 0.4616\n",
      "Epoch 85/10000\n",
      "24/24 [==============================] - 0s 18ms/step - loss: 0.7752 - accuracy: 0.7001 - f1_score: 0.4674 - val_loss: 0.8179 - val_accuracy: 0.7901 - val_f1_score: 0.4559\n",
      "Epoch 86/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.7707 - accuracy: 0.6961 - f1_score: 0.4656 - val_loss: 0.8209 - val_accuracy: 0.7778 - val_f1_score: 0.4604\n",
      "Epoch 87/10000\n",
      "24/24 [==============================] - 0s 16ms/step - loss: 0.7736 - accuracy: 0.7001 - f1_score: 0.4635 - val_loss: 0.8213 - val_accuracy: 0.7716 - val_f1_score: 0.4417\n",
      "Epoch 88/10000\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10000, validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot the losses\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print the F1-score on the validation set\n",
    "_, _, f1_score = model.evaluate(X_test, y_test)\n",
    "print('F1-score on validation set:', f1_score)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
